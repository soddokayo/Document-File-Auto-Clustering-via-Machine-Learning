{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e056a08",
   "metadata": {},
   "source": [
    "# KLUE NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d591ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb408f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3c8a37c1b5462889fbfde576d7c0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/21.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset klue/ner to /Users/jaeha/.cache/huggingface/datasets/klue/ner/1.0.0/e0fc3bc3de3eb03be2c92d72fd04a60ecc71903f821619cb28ca0e1e29e4233e...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57662a2df00147d99983f22961c34a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/21008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset klue downloaded and prepared to /Users/jaeha/.cache/huggingface/datasets/klue/ner/1.0.0/e0fc3bc3de3eb03be2c92d72fd04a60ecc71903f821619cb28ca0e1e29e4233e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01083b96603141849b2d149319272e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('klue', 'ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d60731f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '<중국 후난(湖南)성 창샤(長沙)시 우자링(五家岭)가:LC> 한 시장에서 <14일:DT> <오전 10시 15분:TI>께 칼부림 사건이 일어나 <5명:QT>이 숨지고 <1명:QT>이 부상했다고 <중신넷:OG>이 <14일:DT> 보도했다.',\n",
       " 'tokens': ['중',\n",
       "  '국',\n",
       "  ' ',\n",
       "  '후',\n",
       "  '난',\n",
       "  '(',\n",
       "  '湖',\n",
       "  '南',\n",
       "  ')',\n",
       "  '성',\n",
       "  ' ',\n",
       "  '창',\n",
       "  '샤',\n",
       "  '(',\n",
       "  '長',\n",
       "  '沙',\n",
       "  ')',\n",
       "  '시',\n",
       "  ' ',\n",
       "  '우',\n",
       "  '자',\n",
       "  '링',\n",
       "  '(',\n",
       "  '五',\n",
       "  '家',\n",
       "  '岭',\n",
       "  ')',\n",
       "  '가',\n",
       "  ' ',\n",
       "  '한',\n",
       "  ' ',\n",
       "  '시',\n",
       "  '장',\n",
       "  '에',\n",
       "  '서',\n",
       "  ' ',\n",
       "  '1',\n",
       "  '4',\n",
       "  '일',\n",
       "  ' ',\n",
       "  '오',\n",
       "  '전',\n",
       "  ' ',\n",
       "  '1',\n",
       "  '0',\n",
       "  '시',\n",
       "  ' ',\n",
       "  '1',\n",
       "  '5',\n",
       "  '분',\n",
       "  '께',\n",
       "  ' ',\n",
       "  '칼',\n",
       "  '부',\n",
       "  '림',\n",
       "  ' ',\n",
       "  '사',\n",
       "  '건',\n",
       "  '이',\n",
       "  ' ',\n",
       "  '일',\n",
       "  '어',\n",
       "  '나',\n",
       "  ' ',\n",
       "  '5',\n",
       "  '명',\n",
       "  '이',\n",
       "  ' ',\n",
       "  '숨',\n",
       "  '지',\n",
       "  '고',\n",
       "  ' ',\n",
       "  '1',\n",
       "  '명',\n",
       "  '이',\n",
       "  ' ',\n",
       "  '부',\n",
       "  '상',\n",
       "  '했',\n",
       "  '다',\n",
       "  '고',\n",
       "  ' ',\n",
       "  '중',\n",
       "  '신',\n",
       "  '넷',\n",
       "  '이',\n",
       "  ' ',\n",
       "  '1',\n",
       "  '4',\n",
       "  '일',\n",
       "  ' ',\n",
       "  '보',\n",
       "  '도',\n",
       "  '했',\n",
       "  '다',\n",
       "  '.'],\n",
       " 'ner_tags': [2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  12,\n",
       "  10,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  11,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  8,\n",
       "  9,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  8,\n",
       "  9,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  12,\n",
       "  12,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b7c8b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'tokens', 'ner_tags'],\n",
       "        num_rows: 21008\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'tokens', 'ner_tags'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cb559a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '<엠비씨:OG>에서 사극이 이렇게 망한다는 것을 깨달았다',\n",
       " 'tokens': ['엠',\n",
       "  '비',\n",
       "  '씨',\n",
       "  '에',\n",
       "  '서',\n",
       "  ' ',\n",
       "  '사',\n",
       "  '극',\n",
       "  '이',\n",
       "  ' ',\n",
       "  '이',\n",
       "  '렇',\n",
       "  '게',\n",
       "  ' ',\n",
       "  '망',\n",
       "  '한',\n",
       "  '다',\n",
       "  '는',\n",
       "  ' ',\n",
       "  '것',\n",
       "  '을',\n",
       "  ' ',\n",
       "  '깨',\n",
       "  '달',\n",
       "  '았',\n",
       "  '다'],\n",
       " 'ner_tags': [4,\n",
       "  5,\n",
       "  5,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12,\n",
       "  12]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d15393e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'sentence': Value(dtype='string', id=None),\n",
       "  'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       "  'ner_tags': Sequence(feature=ClassLabel(names=['B-DT', 'I-DT', 'B-LC', 'I-LC', 'B-OG', 'I-OG', 'B-PS', 'I-PS', 'B-QT', 'I-QT', 'B-TI', 'I-TI', 'O'], id=None), length=-1, id=None)},\n",
       " 5000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation'].features, dataset['validation'].num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "435fa04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1125] [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 6, 7, 7, 12, 12, 12, 6, 7, 7, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 6, 7, 7, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 6, 7, 7, 12, 12, 12, 12, 12, 12, 12] 이미 탈락후보로 결정돼 데스매치로 직행한 <강용석:PS>은 \"<최연승:PS>을 (데스매치에) 보내려고 판을 짜는 거 아니냐. 가만히 놔둬. 왜 <하연주:PS>를 네가 설득해”라고 <오현민:PS>을 지적했다.\n",
      "[1126] [4, 5, 5, 5, 5, 5, 5, 12, 12, 12, 12, 12, 12, 12, 6, 12, 12, 12, 12, 12, 4, 5, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 0, 1, 1, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] <새정치민주연합:OG> 소속이었던 <김:PS> 의원은 <경찰:OG>에 체포된 이후인 <26일:DT> 서울시당에 대리인을 통해 탈당계를 제출해 현재는 무소속 상태다.\n",
      "[1127] [0, 1, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 6, 7, 7, 7, 7, 7, 12, 12, 12, 12] <중세:DT>에 시간도 조금 긴면이 있어서 지루할 줄 알았는데 역시 <리들리 스콧:PS>입니다.\n",
      "[1128] [6, 7, 7, 12, 12, 6, 7, 7, 7, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] <존쿠삭:PS>과 <진핵크만:PS>의 연기가 너무 좋았다.\n",
      "[1129] [4, 5, 5, 5, 5, 5, 5, 5, 5, 12, 12, 0, 1, 1, 1, 1, 1, 1, 1, 12, 12, 12, 12, 12, 12, 12, 12, 8, 9, 9, 9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] <기민ㆍ기사당 연합:OG>은 <지난 2009년:DT>보다 득표율이 <8.9%:QT> 포인트(ARD 기준)나 증가해 과반 의석 확보가 안 되더라도 야당과 연정 협상에서 유리한 위치를 차지하게 됐다.\n",
      "[1130] [12, 12, 12, 12, 6, 7, 7, 7, 7, 7, 7, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 4, 5, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 0, 1, 1, 1, 1, 12, 12, 0, 1, 12, 4, 5, 5, 5, 5, 12, 12, 0, 1, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] 격앙한 <앙겔라 메르켈:PS> 총리의 신속한 수사와 철저한 처벌 촉구 메시지가 나온 가운데 <경찰:OG>이 이처럼 수사력을 모으고 있으나, <신년 첫날:DT>인 <1일:DT> <쾰른 경찰:OG>은 <작년:DT> 마지막날 축제가 대체로 평화로웠다는 요지의 성명을 낸 사실이 뒤늦게 알려져 시민들의 비판을 샀다.\n",
      "[1131] [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 0, 1, 1, 1, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 8, 9, 9, 9, 9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 8, 9, 9, 9, 12, 12, 12, 12, 12] 블록버스터 사극의 시작, <조선왕조:DT>를 다루지 않은 첫 사극, 전무후무한 <200부작:QT>의 사극이면서도 막장으로 흐르지 않고 끝까지 시청률 <40프로:QT>대 유지.\n",
      "[1132] [6, 7, 7, 12, 12, 12, 12, 12, 12, 12, 12, 0, 1, 1, 1, 12, 12, 12, 12, 12, 6, 7, 7, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] <육성재:PS> 씨가 '학교 <2015:DT>'에서 '<공태광:PS>'역을 연기할 수 있었던 데는 그의 욱하는 성격이 한 몫한 것으로 전해졌다.\n",
      "[1133] [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 12, 6, 7, 12, 12, 6, 7, 7, 7, 7, 7, 7, 12, 12, 12, 8, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] <중학교 고등학교시절:DT> <세나:PS>와 <프로스트 만쎌:PS>이 F<1:QT>레이스를 다투던 기사를 보던게 생생하다..\n",
      "[1134] [12, 12, 12, 4, 5, 5, 12, 12, 12, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] 이어 <경찰청:OG>에 '<불법폭력시위 대응 태스크포스:OG>'를 설치하고 모든 지방경찰청에 '<불법폭력시위 수사본부:OG>'를 설치하겠다고 밝혔다.\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "lines = []\n",
    "for i in range(int(dataset['validation'].num_rows)): # 9107개\n",
    "    label = dataset['validation'][i]['ner_tags']\n",
    "    line = dataset['validation'][i]['sentence']\n",
    "    labels.append(label)\n",
    "    lines.append(line)\n",
    "    \n",
    "for i in range(1125, 1135):\n",
    "    print(\"[\"+str(i)+\"]\", labels[i], lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a3e0e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['B-DT', 'I-DT', 'B-LC', 'I-LC', 'B-OG', 'I-OG', 'B-PS', 'I-PS', 'B-QT', 'I-QT', 'B-TI', 'I-TI', 'O'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_feature = dataset[\"train\"].features[\"ner_tags\"]\n",
    "ner_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "abf0af99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13,\n",
       " ['B-DT',\n",
       "  'I-DT',\n",
       "  'B-LC',\n",
       "  'I-LC',\n",
       "  'B-OG',\n",
       "  'I-OG',\n",
       "  'B-PS',\n",
       "  'I-PS',\n",
       "  'B-QT',\n",
       "  'I-QT',\n",
       "  'B-TI',\n",
       "  'I-TI',\n",
       "  'O'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = ner_feature.feature.names\n",
    "len(label_names), label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "89240f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쇼,', ,참,가, ,레,이,싱,모,델, ,허,윤,미, ,씨,의, ,귀,요,미, ,셀,카,입,니,다,.,\n",
      "O,O,O,O,O,O,O,O,O,O,O,O,B-PS,I-PS,I-PS,O,O,O,O,O,O,O,O,O,O,O,O,O,O,\n",
      "[12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 6, 7, 7, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n"
     ]
    }
   ],
   "source": [
    "words = dataset[\"train\"][5][\"tokens\"][9:]\n",
    "labels = dataset[\"train\"][5][\"ner_tags\"][9:]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    line1 += word + \",\"\n",
    "    line2 += full_label + \",\"\n",
    "    \n",
    "print(line1)\n",
    "print(line2)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdae301a",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e74af48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"klue/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3915d76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3e596a52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '특', '히', '영', '동', '고', '속', '도', '로', '강', '릉', '방', '향', '문', '막', '휴', '게', '소', '에', '서', '만', '종', '분', '기', '점', '까', '지', '5', '㎞', '구', '간', '에', '는', '승', '용', '차', '전', '용', '임', '시', '갓', '길', '차', '로', '제', '를', '운', '영', '하', '기', '로', '했', '다', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(dataset[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "print(inputs.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e05ba4f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 3, 4, 5, 6, 7, 8, 10, 11, 13, 14, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 32, 33, 35, 36, 37, 38, 40, 41, 42, 44, 45, 47, 48, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 63, 64, 65, None]\n"
     ]
    }
   ],
   "source": [
    "print(inputs.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "16710601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if (label % 2 == 0) and (label != 12):\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "25888736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 [12, 12, 12, 2, 3, 3, 3, 3, 3, 12, 2, 3, 12, 12, 12, 12, 2, 3, 3, 3, 3, 12, 12, 12, 2, 3, 3, 3, 3, 12, 12, 12, 8, 9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
      "55 [-100, 12, 12, 2, 3, 3, 3, 3, 3, 2, 3, 12, 12, 2, 3, 3, 3, 3, 12, 12, 2, 3, 3, 3, 3, 12, 12, 8, 9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = dataset['train'][0]['ner_tags']\n",
    "word_ids = inputs.word_ids()\n",
    "print(len(labels), labels)\n",
    "print(len(align_labels_with_tokens(labels, word_ids)), align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f8ae416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples['ner_tags']\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "        \n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6b794c49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 21008\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    ")\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c6d1e6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 1813, 1969, 1437, 856, 594, 1283, 848, 991, 553, 1026, 1129, 1904, 1091, 1037, 1956, 578, 1282, 1421, 1258, 1038, 1558, 1175, 645, 1540, 653, 1583, 25, 207, 615, 545, 1421, 793, 1324, 1468, 1632, 1537, 1468, 1510, 1325, 551, 647, 1632, 991, 1545, 1022, 1471, 1437, 1889, 645, 991, 1902, 809, 18, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 12, 12, 2, 3, 3, 3, 3, 3, 2, 3, 12, 12, 2, 3, 3, 3, 3, 12, 12, 2, 3, 3, 3, 3, 12, 12, 8, 9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, -100]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "167ec9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "033cf209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-100,   12,   12,    2,    3,    3,    3,    3,    3,    2,    3,   12,\n",
      "           12,    2,    3,    3,    3,    3,   12,   12,    2,    3,    3,    3,\n",
      "            3,   12,   12,    8,    9,   12,   12,   12,   12,   12,   12,   12,\n",
      "           12,   12,   12,   12,   12,   12,   12,   12,   12,   12,   12,   12,\n",
      "           12,   12,   12,   12,   12,   12, -100],\n",
      "        [-100,    8,    9,    9,   12,   12,   12,   12,   12,   12,   12,   12,\n",
      "           12,   12,   12,   12,   12,   12,   12, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets['train'][i] for i in range(2)])\n",
    "print(batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "18f82b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': '<한군데:QT>서 필름을 너무 낭비한 작품입니다.', 'tokens': ['한', '군', '데', '서', ' ', '필', '름', '을', ' ', '너', '무', ' ', '낭', '비', '한', ' ', '작', '품', '입', '니', '다', '.'], 'ner_tags': [8, 9, 9, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][1]) # 위에서 길이가 짧으면 패딩 잘 되는게 보이죵? 두번째게 이친구임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "85c3cbb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from seqeval) (1.21.5)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from seqeval) (1.1.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.21.3->seqeval) (2.2.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: multiprocess in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: pandas in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from evaluate) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from evaluate) (2.12.0)\n",
      "Requirement already satisfied: xxhash in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from evaluate) (1.21.5)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from evaluate) (2022.7.1)\n",
      "Requirement already satisfied: packaging in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: dill in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from evaluate) (0.13.2)\n",
      "Requirement already satisfied: responses<0.19 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (12.0.0)\n",
      "Requirement already satisfied: aiohttp in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
      "Requirement already satisfied: filelock in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from pandas->evaluate) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/jaeha/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6b51aa1e264141a08b95f01949147c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install seqeval\n",
    "!pip install evaluate\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3b120a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'B-LC', 'I-LC', 'I-LC', 'I-LC', 'I-LC', 'I-LC', 'O', 'B-LC', 'I-LC', 'O', 'O', 'O', 'O', 'B-LC', 'I-LC', 'I-LC', 'I-LC', 'I-LC', 'O', 'O', 'O', 'B-LC', 'I-LC', 'I-LC', 'I-LC', 'I-LC', 'O', 'O', 'O', 'B-QT', 'I-QT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "labels = dataset[\"train\"][0][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a3245cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LC': {'precision': 0.6,\n",
       "  'recall': 0.75,\n",
       "  'f1': 0.6666666666666665,\n",
       "  'number': 4},\n",
       " 'QT': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 0.6666666666666666,\n",
       " 'overall_recall': 0.8,\n",
       " 'overall_f1': 0.7272727272727272,\n",
       " 'overall_accuracy': 0.9848484848484849}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = labels.copy()\n",
    "predictions[4] = \"O\"\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d3403f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "20d36482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'B-DT', 1: 'I-DT', 2: 'B-LC', 3: 'I-LC', 4: 'B-OG', 5: 'I-OG', 6: 'B-PS', 7: 'I-PS', 8: 'B-QT', 9: 'I-QT', 10: 'B-TI', 11: 'I-TI', 12: 'O'} {'B-DT': 0, 'I-DT': 1, 'B-LC': 2, 'I-LC': 3, 'B-OG': 4, 'I-OG': 5, 'B-PS': 6, 'I-PS': 7, 'B-QT': 8, 'I-QT': 9, 'B-TI': 10, 'I-TI': 11, 'O': 12}\n"
     ]
    }
   ],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "print(id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7ad6bccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fd6b7eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7ad81805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/jaeha/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5884d5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"klue-roberta-large-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a166cb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/soddokayo/klue-roberta-large-ner into local empty directory.\n",
      "/Users/jaeha/opt/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 21008\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7878\n",
      "  Number of trainable parameters = 110037517\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7878' max='7878' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7878/7878 5:41:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.158500</td>\n",
       "      <td>0.164793</td>\n",
       "      <td>0.751670</td>\n",
       "      <td>0.749877</td>\n",
       "      <td>0.750772</td>\n",
       "      <td>0.948921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.109200</td>\n",
       "      <td>0.145685</td>\n",
       "      <td>0.777602</td>\n",
       "      <td>0.790910</td>\n",
       "      <td>0.784199</td>\n",
       "      <td>0.955698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.071400</td>\n",
       "      <td>0.143224</td>\n",
       "      <td>0.788199</td>\n",
       "      <td>0.810479</td>\n",
       "      <td>0.799184</td>\n",
       "      <td>0.959060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to klue-roberta-large-ner/checkpoint-2626\n",
      "Configuration saved in klue-roberta-large-ner/checkpoint-2626/config.json\n",
      "Model weights saved in klue-roberta-large-ner/checkpoint-2626/pytorch_model.bin\n",
      "tokenizer config file saved in klue-roberta-large-ner/checkpoint-2626/tokenizer_config.json\n",
      "Special tokens file saved in klue-roberta-large-ner/checkpoint-2626/special_tokens_map.json\n",
      "tokenizer config file saved in klue-roberta-large-ner/tokenizer_config.json\n",
      "Special tokens file saved in klue-roberta-large-ner/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to klue-roberta-large-ner/checkpoint-5252\n",
      "Configuration saved in klue-roberta-large-ner/checkpoint-5252/config.json\n",
      "Model weights saved in klue-roberta-large-ner/checkpoint-5252/pytorch_model.bin\n",
      "tokenizer config file saved in klue-roberta-large-ner/checkpoint-5252/tokenizer_config.json\n",
      "Special tokens file saved in klue-roberta-large-ner/checkpoint-5252/special_tokens_map.json\n",
      "tokenizer config file saved in klue-roberta-large-ner/tokenizer_config.json\n",
      "Special tokens file saved in klue-roberta-large-ner/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to klue-roberta-large-ner/checkpoint-7878\n",
      "Configuration saved in klue-roberta-large-ner/checkpoint-7878/config.json\n",
      "Model weights saved in klue-roberta-large-ner/checkpoint-7878/pytorch_model.bin\n",
      "tokenizer config file saved in klue-roberta-large-ner/checkpoint-7878/tokenizer_config.json\n",
      "Special tokens file saved in klue-roberta-large-ner/checkpoint-7878/special_tokens_map.json\n",
      "tokenizer config file saved in klue-roberta-large-ner/tokenizer_config.json\n",
      "Special tokens file saved in klue-roberta-large-ner/special_tokens_map.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7878, training_loss=0.13683060348018772, metrics={'train_runtime': 20484.0854, 'train_samples_per_second': 3.077, 'train_steps_per_second': 0.385, 'total_flos': 2529804225374064.0, 'train_loss': 0.13683060348018772, 'epoch': 3.0})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a4f82c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to klue-roberta-large-ner\n",
      "Configuration saved in klue-roberta-large-ner/config.json\n",
      "Model weights saved in klue-roberta-large-ner/pytorch_model.bin\n",
      "tokenizer config file saved in klue-roberta-large-ner/tokenizer_config.json\n",
      "Special tokens file saved in klue-roberta-large-ner/special_tokens_map.json\n",
      "To https://huggingface.co/soddokayo/klue-roberta-large-ner\n",
      "   5cc104b..8271123  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad86444",
   "metadata": {},
   "source": [
    "## customize train loop -> pass!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20272235",
   "metadata": {},
   "source": [
    "# Using the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "752b44ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file klue-roberta-large-ner/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"klue-roberta-large-ner\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B-DT\",\n",
      "    \"1\": \"I-DT\",\n",
      "    \"2\": \"B-LC\",\n",
      "    \"3\": \"I-LC\",\n",
      "    \"4\": \"B-OG\",\n",
      "    \"5\": \"I-OG\",\n",
      "    \"6\": \"B-PS\",\n",
      "    \"7\": \"I-PS\",\n",
      "    \"8\": \"B-QT\",\n",
      "    \"9\": \"I-QT\",\n",
      "    \"10\": \"B-TI\",\n",
      "    \"11\": \"I-TI\",\n",
      "    \"12\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-DT\": 0,\n",
      "    \"B-LC\": 2,\n",
      "    \"B-OG\": 4,\n",
      "    \"B-PS\": 6,\n",
      "    \"B-QT\": 8,\n",
      "    \"B-TI\": 10,\n",
      "    \"I-DT\": 1,\n",
      "    \"I-LC\": 3,\n",
      "    \"I-OG\": 5,\n",
      "    \"I-PS\": 7,\n",
      "    \"I-QT\": 9,\n",
      "    \"I-TI\": 11,\n",
      "    \"O\": 12\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file klue-roberta-large-ner/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"klue-roberta-large-ner\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B-DT\",\n",
      "    \"1\": \"I-DT\",\n",
      "    \"2\": \"B-LC\",\n",
      "    \"3\": \"I-LC\",\n",
      "    \"4\": \"B-OG\",\n",
      "    \"5\": \"I-OG\",\n",
      "    \"6\": \"B-PS\",\n",
      "    \"7\": \"I-PS\",\n",
      "    \"8\": \"B-QT\",\n",
      "    \"9\": \"I-QT\",\n",
      "    \"10\": \"B-TI\",\n",
      "    \"11\": \"I-TI\",\n",
      "    \"12\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-DT\": 0,\n",
      "    \"B-LC\": 2,\n",
      "    \"B-OG\": 4,\n",
      "    \"B-PS\": 6,\n",
      "    \"B-QT\": 8,\n",
      "    \"B-TI\": 10,\n",
      "    \"I-DT\": 1,\n",
      "    \"I-LC\": 3,\n",
      "    \"I-OG\": 5,\n",
      "    \"I-PS\": 7,\n",
      "    \"I-QT\": 9,\n",
      "    \"I-TI\": 11,\n",
      "    \"O\": 12\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file klue-roberta-large-ner/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForTokenClassification.\n",
      "\n",
      "All the weights of RobertaForTokenClassification were initialized from the model checkpoint at klue-roberta-large-ner.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForTokenClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PS',\n",
       "  'score': 0.9967304,\n",
       "  'word': '곽두팔',\n",
       "  'start': 7,\n",
       "  'end': 10},\n",
       " {'entity_group': 'TI',\n",
       "  'score': 0.9932538,\n",
       "  'word': '8시',\n",
       "  'start': 18,\n",
       "  'end': 20},\n",
       " {'entity_group': 'LC',\n",
       "  'score': 0.9779833,\n",
       "  'word': '서울 여의도 63빌딩',\n",
       "  'start': 22,\n",
       "  'end': 33}]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"klue-roberta-large-ner\"\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "token_classifier(\"안녕하세요, 곽두팔입니다. 오늘 8시에 서울 여의도 63빌딩에서 뵙죠.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fb4dddfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file klue-roberta-large-ner/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"klue-roberta-large-ner\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B-DT\",\n",
      "    \"1\": \"I-DT\",\n",
      "    \"2\": \"B-LC\",\n",
      "    \"3\": \"I-LC\",\n",
      "    \"4\": \"B-OG\",\n",
      "    \"5\": \"I-OG\",\n",
      "    \"6\": \"B-PS\",\n",
      "    \"7\": \"I-PS\",\n",
      "    \"8\": \"B-QT\",\n",
      "    \"9\": \"I-QT\",\n",
      "    \"10\": \"B-TI\",\n",
      "    \"11\": \"I-TI\",\n",
      "    \"12\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-DT\": 0,\n",
      "    \"B-LC\": 2,\n",
      "    \"B-OG\": 4,\n",
      "    \"B-PS\": 6,\n",
      "    \"B-QT\": 8,\n",
      "    \"B-TI\": 10,\n",
      "    \"I-DT\": 1,\n",
      "    \"I-LC\": 3,\n",
      "    \"I-OG\": 5,\n",
      "    \"I-PS\": 7,\n",
      "    \"I-QT\": 9,\n",
      "    \"I-TI\": 11,\n",
      "    \"O\": 12\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file klue-roberta-large-ner/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"klue-roberta-large-ner\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B-DT\",\n",
      "    \"1\": \"I-DT\",\n",
      "    \"2\": \"B-LC\",\n",
      "    \"3\": \"I-LC\",\n",
      "    \"4\": \"B-OG\",\n",
      "    \"5\": \"I-OG\",\n",
      "    \"6\": \"B-PS\",\n",
      "    \"7\": \"I-PS\",\n",
      "    \"8\": \"B-QT\",\n",
      "    \"9\": \"I-QT\",\n",
      "    \"10\": \"B-TI\",\n",
      "    \"11\": \"I-TI\",\n",
      "    \"12\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-DT\": 0,\n",
      "    \"B-LC\": 2,\n",
      "    \"B-OG\": 4,\n",
      "    \"B-PS\": 6,\n",
      "    \"B-QT\": 8,\n",
      "    \"B-TI\": 10,\n",
      "    \"I-DT\": 1,\n",
      "    \"I-LC\": 3,\n",
      "    \"I-OG\": 5,\n",
      "    \"I-PS\": 7,\n",
      "    \"I-QT\": 9,\n",
      "    \"I-TI\": 11,\n",
      "    \"O\": 12\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file klue-roberta-large-ner/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForTokenClassification.\n",
      "\n",
      "All the weights of RobertaForTokenClassification were initialized from the model checkpoint at klue-roberta-large-ner.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForTokenClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PS',\n",
       "  'score': 0.9967304,\n",
       "  'word': '곽두팔',\n",
       "  'start': 7,\n",
       "  'end': 10},\n",
       " {'entity_group': 'TI',\n",
       "  'score': 0.9932538,\n",
       "  'word': '8시',\n",
       "  'start': 18,\n",
       "  'end': 20},\n",
       " {'entity_group': 'LC',\n",
       "  'score': 0.9779833,\n",
       "  'word': '서울 여의도 63빌딩',\n",
       "  'start': 22,\n",
       "  'end': 33}]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\n",
    "    \"ner\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "ner(\"안녕하세요, 곽두팔입니다. 오늘 8시에 서울 여의도 63빌딩에서 뵙죠.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747f4e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ba0455c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/jaeha/.cache/huggingface/hub/models--soddokayo--klue-roberta-large-ner/snapshots/82711236b3a31d2514239f0ebab8e1f68cd678d4/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"soddokayo/klue-roberta-large-ner\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B-DT\",\n",
      "    \"1\": \"I-DT\",\n",
      "    \"2\": \"B-LC\",\n",
      "    \"3\": \"I-LC\",\n",
      "    \"4\": \"B-OG\",\n",
      "    \"5\": \"I-OG\",\n",
      "    \"6\": \"B-PS\",\n",
      "    \"7\": \"I-PS\",\n",
      "    \"8\": \"B-QT\",\n",
      "    \"9\": \"I-QT\",\n",
      "    \"10\": \"B-TI\",\n",
      "    \"11\": \"I-TI\",\n",
      "    \"12\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-DT\": 0,\n",
      "    \"B-LC\": 2,\n",
      "    \"B-OG\": 4,\n",
      "    \"B-PS\": 6,\n",
      "    \"B-QT\": 8,\n",
      "    \"B-TI\": 10,\n",
      "    \"I-DT\": 1,\n",
      "    \"I-LC\": 3,\n",
      "    \"I-OG\": 5,\n",
      "    \"I-PS\": 7,\n",
      "    \"I-QT\": 9,\n",
      "    \"I-TI\": 11,\n",
      "    \"O\": 12\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/jaeha/.cache/huggingface/hub/models--soddokayo--klue-roberta-large-ner/snapshots/82711236b3a31d2514239f0ebab8e1f68cd678d4/pytorch_model.bin\n",
      "Some weights of the model checkpoint at soddokayo/klue-roberta-large-ner were not used when initializing RobertaModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at soddokayo/klue-roberta-large-ner and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at /Users/jaeha/.cache/huggingface/hub/models--soddokayo--klue-roberta-large-ner/snapshots/82711236b3a31d2514239f0ebab8e1f68cd678d4/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"soddokayo/klue-roberta-large-ner\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B-DT\",\n",
      "    \"1\": \"I-DT\",\n",
      "    \"2\": \"B-LC\",\n",
      "    \"3\": \"I-LC\",\n",
      "    \"4\": \"B-OG\",\n",
      "    \"5\": \"I-OG\",\n",
      "    \"6\": \"B-PS\",\n",
      "    \"7\": \"I-PS\",\n",
      "    \"8\": \"B-QT\",\n",
      "    \"9\": \"I-QT\",\n",
      "    \"10\": \"B-TI\",\n",
      "    \"11\": \"I-TI\",\n",
      "    \"12\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-DT\": 0,\n",
      "    \"B-LC\": 2,\n",
      "    \"B-OG\": 4,\n",
      "    \"B-PS\": 6,\n",
      "    \"B-QT\": 8,\n",
      "    \"B-TI\": 10,\n",
      "    \"I-DT\": 1,\n",
      "    \"I-LC\": 3,\n",
      "    \"I-OG\": 5,\n",
      "    \"I-PS\": 7,\n",
      "    \"I-QT\": 9,\n",
      "    \"I-TI\": 11,\n",
      "    \"O\": 12\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/jaeha/.cache/huggingface/hub/models--soddokayo--klue-roberta-large-ner/snapshots/82711236b3a31d2514239f0ebab8e1f68cd678d4/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"soddokayo/klue-roberta-large-ner\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"B-DT\",\n",
      "    \"1\": \"I-DT\",\n",
      "    \"2\": \"B-LC\",\n",
      "    \"3\": \"I-LC\",\n",
      "    \"4\": \"B-OG\",\n",
      "    \"5\": \"I-OG\",\n",
      "    \"6\": \"B-PS\",\n",
      "    \"7\": \"I-PS\",\n",
      "    \"8\": \"B-QT\",\n",
      "    \"9\": \"I-QT\",\n",
      "    \"10\": \"B-TI\",\n",
      "    \"11\": \"I-TI\",\n",
      "    \"12\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-DT\": 0,\n",
      "    \"B-LC\": 2,\n",
      "    \"B-OG\": 4,\n",
      "    \"B-PS\": 6,\n",
      "    \"B-QT\": 8,\n",
      "    \"B-TI\": 10,\n",
      "    \"I-DT\": 1,\n",
      "    \"I-LC\": 3,\n",
      "    \"I-OG\": 5,\n",
      "    \"I-PS\": 7,\n",
      "    \"I-QT\": 9,\n",
      "    \"I-TI\": 11,\n",
      "    \"O\": 12\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/jaeha/.cache/huggingface/hub/models--soddokayo--klue-roberta-large-ner/snapshots/82711236b3a31d2514239f0ebab8e1f68cd678d4/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForTokenClassification.\n",
      "\n",
      "All the weights of RobertaForTokenClassification were initialized from the model checkpoint at soddokayo/klue-roberta-large-ner.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForTokenClassification for predictions without further training.\n",
      "loading file vocab.txt from cache at /Users/jaeha/.cache/huggingface/hub/models--soddokayo--klue-roberta-large-ner/snapshots/82711236b3a31d2514239f0ebab8e1f68cd678d4/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/jaeha/.cache/huggingface/hub/models--soddokayo--klue-roberta-large-ner/snapshots/82711236b3a31d2514239f0ebab8e1f68cd678d4/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/jaeha/.cache/huggingface/hub/models--soddokayo--klue-roberta-large-ner/snapshots/82711236b3a31d2514239f0ebab8e1f68cd678d4/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/jaeha/.cache/huggingface/hub/models--soddokayo--klue-roberta-large-ner/snapshots/82711236b3a31d2514239f0ebab8e1f68cd678d4/tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PS',\n",
       "  'score': 0.99689674,\n",
       "  'index': 5,\n",
       "  'word': '곽',\n",
       "  'start': 7,\n",
       "  'end': 8},\n",
       " {'entity': 'I-PS',\n",
       "  'score': 0.99669904,\n",
       "  'index': 6,\n",
       "  'word': '##두',\n",
       "  'start': 8,\n",
       "  'end': 9},\n",
       " {'entity': 'I-PS',\n",
       "  'score': 0.9961738,\n",
       "  'index': 7,\n",
       "  'word': '##팔',\n",
       "  'start': 9,\n",
       "  'end': 10}]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "checkpoint = \"soddokayo/klue-roberta-large-ner\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "ner = pipeline(\"ner\", model=checkpoint)#, aggregation_strategy=\"simple\")\n",
    "ner(\"안녕하세요, 곽두팔입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321032d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
