{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e3de97a",
   "metadata": {},
   "source": [
    "# Torchtext 라이브러리로 텍스트 분류하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70c7b74",
   "metadata": {},
   "source": [
    "이 튜토리얼에서는 torchtext 라이브러리를 사용하여 어떻게 텍스트 분류 분석을 위한 데이터셋을 만드는지를 살펴보겠습니다. 다음과 같은 내용들을 알게 됩니다:\n",
    "\n",
    "- 반복자(iterator)로 가공되지 않은 데이터(raw data)에 접근하기\n",
    "- 가공되지 않은 텍스트 문장들을 모델 학습에 사용할 수 있는 torch.Tensor 로 변환하는 데이터 처리 파이프라인 만들기\n",
    "- torch.utils.data.DataLoader 를 사용하여 데이터를 섞고 반복하기(shuffle and iterate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f845f",
   "metadata": {},
   "source": [
    "## 기초 데이터셋 반복자(raw data iterator)에 접근하기\n",
    "\n",
    "torchtext 라이브러리는 가공되지 않은 텍스트 문장들을 만드는(yield) 몇 가지 기초 데이터셋 반복자(raw dataset iterator)를 제공합니다. 예를 들어, `AG_NEWS` 데이터셋 반복자는 레이블(label)과 문장의 튜플(tuple) 형태로 가공되지 않은 데이터를 만듭니다.\n",
    "\n",
    "torchtext 데이터셋에 접근하기 전에, https://github.com/pytorch/data 을 참고하여 torchdata를 설치하시기 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a007a0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "train_iter = iter(AG_NEWS(split='train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32ca9d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e742d70",
   "metadata": {},
   "source": [
    "## 데이터 처리 파이프라인 준비하기\n",
    "\n",
    "어휘집(vocab), 단어 벡터(word vector), 토크나이저(tokenizer)를 포함하여 torchtext 라이브러리의 가장 기본적인 구성요소를 재검토했습니다. 이들은 가공되지 않은 텍스트 문자열에 대한 기본적인 데이터 처리 빌딩 블록(data processing building block)입니다.\n",
    "\n",
    "다음은 토크나이저 및 어휘집을 사용한 일반적인 NLP 데이터 처리의 예입니다. 첫번째 단계는 가공되지 않은 학습 데이터셋으로 어휘집을 만드는 것입니다. 여기에서는 토큰의 목록 또는 반복자를 받는 내장(built-in) 팩토리 함수(factory function) *build_vocab_from_iterator*를 사용합니다. 사용자는 어휘집에 추가할 특수 기호(special symbl) 같은 것들을 전달할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "        \n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea89d06c",
   "metadata": {},
   "source": [
    "어휘집 블록(vocabulary block)은 토큰 목록을 정수로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0802f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab(['here', 'is', 'an', 'example'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf2fcbb",
   "metadata": {},
   "source": [
    "토크나이저와 어휘집을 갖춘 텍스트 처리 파이프라인을 준비합니다. 텍스트 파이프라인 레이블(label) 파이프라인은 데이터셋 반복자로부터 얻어온 가공되지 않은 문장 데이터를 처리하기 위해 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e59b47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1184636b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[475, 21, 30, 5297]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline('here is an example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9610bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline('10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b227b",
   "metadata": {},
   "source": [
    "## 데이터 배치(batch)와 반복자 생성하기\n",
    "\n",
    "**torch.utils.data.DataLoader**를 권장합니다. 이는 `getitem()`과 `len()` 프로토콜을 구현한 맵 형태(map-style)의 데이터셋으로 동작하며, 맵(map)처럼 인덱스/키로 데이터 샘플을 얻어옵니다. 또한, 셔플(shuffle) 인자를 `False`로 설정하면 순회 가능한(iterable) 데이터셋처럼 동작합니다.\n",
    "\n",
    "모델을 보내기 전, `collate_fn` 함수는 `Datalodaer`로부터 생성된 샘플 배치로 동작합니다. `collate_fn`의 입력은 `Dataloader`에 배치 크기(batch size)가 있는 배치(batch) 데이터이며, `collate_fn`은 이를 미리 선언된 데이터 처리 파이프라인에 따라 처리합니다. `collate_fn`이 최상위 수준으로 정의(top level def)되었는지 확인합니다. 이렇게 하면 모든 워커에서 이 함수를 사용할 수 있습니다.\n",
    "\n",
    "아래 예제에서, 주어진(original) 데이터 배치의 항목들은 리스트(list)에 담긴(pack) 뒤 `nn.EmbeddingBag`의 입력을 위한 하나의 tensor로 합쳐(concatenate)집니다. 오프셋(offset)은 텍스트 tensor에서 개별 시퀀스 시작 인덱스를 표현하기 위한 구분자(delimiter) tensor입니다. 레이블(label)은 개별 텍스트 항목의 레이블을 저장하는 tensor입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82039ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e5c95d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1080469d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0848e6e5",
   "metadata": {},
   "source": [
    "## 모델 정의하기\n",
    "\n",
    "모델은 **nn.EmbeddingBag**레이어와 분류(classification) 목적을 위한 선형 레이어로 구성됩니다. 기본 모드가 <평균(mean)>인 `nn.EmbeddingBag`은 임베딩들의 <가방(bag)>의 평균 값을 계산합니다. 이때 텍스트(text) 항목들은 각기 그 길이가 다를 수 있지만, `nn.EmbeddingBag` 모듈은 텍스트의 길이를 오프셋(offset)으로 저장하고 있으므로 패딩(padding)이 필요하지는 않습니다.\n",
    "\n",
    "덧붙여서, `nn.EmbeddingBag`은 임베딩의 평균을 즉시 계산하기 때문에, tensor들의 시퀀스를 처리할 때 성능 및 메모리 효율성 측면에서의 장점도 갖고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "054015cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b0d71",
   "metadata": {},
   "source": [
    "## 인스턴스 생성하기\n",
    "\n",
    "`AG_NEWS`데이터셋에는 4종류의 레이블이 존재하므로 클래스의 개수도 4개입니다.\n",
    "```\n",
    "1: World (세계)\n",
    "2: Sports (스포츠)\n",
    "3: Business (경제)\n",
    "4: Sci/Tec (과학/기술)\n",
    "```\n",
    "임베딩 차원이 64인 모델을 만듭니다. 어휘집의 크기(Vocab size)는 어휘집(vocab)의 길이와 같습니다. 클래스의 개수는 레이블의 개수와 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48471c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a6e40",
   "metadata": {},
   "source": [
    "## 모델을 학습하고 결과를 평가하는 함수 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a3c595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(datalodaer):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                 '| accuracy {:8.3f}'.format(epoch, idx, 1782,#len(dataloader),\n",
    "                                            total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13c081f",
   "metadata": {},
   "source": [
    "## 데이터셋을 분할하고 모델 수행하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d5ec102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1782 batches | accuracy    0.967\n",
      "| epoch   1 |  1000/ 1782 batches | accuracy    0.948\n",
      "| epoch   1 |  1500/ 1782 batches | accuracy    0.952\n",
      "| epoch   1 |  2000/ 1782 batches | accuracy    0.953\n",
      "| epoch   1 |  2500/ 1782 batches | accuracy    0.958\n",
      "| epoch   1 |  3000/ 1782 batches | accuracy    0.968\n",
      "| epoch   1 |  3500/ 1782 batches | accuracy    0.963\n",
      "| epoch   1 |  4000/ 1782 batches | accuracy    0.963\n",
      "| epoch   1 |  4500/ 1782 batches | accuracy    0.958\n",
      "| epoch   1 |  5000/ 1782 batches | accuracy    0.968\n",
      "| epoch   1 |  5500/ 1782 batches | accuracy    0.955\n",
      "| epoch   1 |  6000/ 1782 batches | accuracy    0.954\n",
      "| epoch   1 |  6500/ 1782 batches | accuracy    0.955\n",
      "| epoch   1 |  7000/ 1782 batches | accuracy    0.960\n",
      "| epoch   1 |  7500/ 1782 batches | accuracy    0.954\n",
      "| epoch   1 |  8000/ 1782 batches | accuracy    0.952\n",
      "| epoch   1 |  8500/ 1782 batches | accuracy    0.963\n",
      "| epoch   1 |  9000/ 1782 batches | accuracy    0.958\n",
      "| epoch   1 |  9500/ 1782 batches | accuracy    0.967\n",
      "| epoch   1 | 10000/ 1782 batches | accuracy    0.978\n",
      "| epoch   1 | 10500/ 1782 batches | accuracy    0.981\n",
      "| epoch   1 | 11000/ 1782 batches | accuracy    0.965\n",
      "| epoch   1 | 11500/ 1782 batches | accuracy    0.963\n",
      "| epoch   1 | 12000/ 1782 batches | accuracy    0.964\n",
      "| epoch   1 | 12500/ 1782 batches | accuracy    0.962\n",
      "| epoch   1 | 13000/ 1782 batches | accuracy    0.971\n",
      "| epoch   1 | 13500/ 1782 batches | accuracy    0.966\n",
      "| epoch   1 | 14000/ 1782 batches | accuracy    0.962\n",
      "| epoch   1 | 14500/ 1782 batches | accuracy    0.965\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  9.43s | valid accuracy    0.958\n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1782 batches | accuracy    0.953\n",
      "| epoch   2 |  1000/ 1782 batches | accuracy    0.955\n",
      "| epoch   2 |  1500/ 1782 batches | accuracy    0.955\n",
      "| epoch   2 |  2000/ 1782 batches | accuracy    0.959\n",
      "| epoch   2 |  2500/ 1782 batches | accuracy    0.960\n",
      "| epoch   2 |  3000/ 1782 batches | accuracy    0.968\n",
      "| epoch   2 |  3500/ 1782 batches | accuracy    0.965\n",
      "| epoch   2 |  4000/ 1782 batches | accuracy    0.963\n",
      "| epoch   2 |  4500/ 1782 batches | accuracy    0.960\n",
      "| epoch   2 |  5000/ 1782 batches | accuracy    0.970\n",
      "| epoch   2 |  5500/ 1782 batches | accuracy    0.959\n",
      "| epoch   2 |  6000/ 1782 batches | accuracy    0.958\n",
      "| epoch   2 |  6500/ 1782 batches | accuracy    0.958\n",
      "| epoch   2 |  7000/ 1782 batches | accuracy    0.962\n",
      "| epoch   2 |  7500/ 1782 batches | accuracy    0.958\n",
      "| epoch   2 |  8000/ 1782 batches | accuracy    0.954\n",
      "| epoch   2 |  8500/ 1782 batches | accuracy    0.965\n",
      "| epoch   2 |  9000/ 1782 batches | accuracy    0.960\n",
      "| epoch   2 |  9500/ 1782 batches | accuracy    0.972\n",
      "| epoch   2 | 10000/ 1782 batches | accuracy    0.978\n",
      "| epoch   2 | 10500/ 1782 batches | accuracy    0.981\n",
      "| epoch   2 | 11000/ 1782 batches | accuracy    0.968\n",
      "| epoch   2 | 11500/ 1782 batches | accuracy    0.964\n",
      "| epoch   2 | 12000/ 1782 batches | accuracy    0.966\n",
      "| epoch   2 | 12500/ 1782 batches | accuracy    0.964\n",
      "| epoch   2 | 13000/ 1782 batches | accuracy    0.971\n",
      "| epoch   2 | 13500/ 1782 batches | accuracy    0.967\n",
      "| epoch   2 | 14000/ 1782 batches | accuracy    0.964\n",
      "| epoch   2 | 14500/ 1782 batches | accuracy    0.966\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  9.28s | valid accuracy    0.960\n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1782 batches | accuracy    0.953\n",
      "| epoch   3 |  1000/ 1782 batches | accuracy    0.958\n",
      "| epoch   3 |  1500/ 1782 batches | accuracy    0.956\n",
      "| epoch   3 |  2000/ 1782 batches | accuracy    0.961\n",
      "| epoch   3 |  2500/ 1782 batches | accuracy    0.961\n",
      "| epoch   3 |  3000/ 1782 batches | accuracy    0.968\n",
      "| epoch   3 |  3500/ 1782 batches | accuracy    0.965\n",
      "| epoch   3 |  4000/ 1782 batches | accuracy    0.965\n",
      "| epoch   3 |  4500/ 1782 batches | accuracy    0.964\n",
      "| epoch   3 |  5000/ 1782 batches | accuracy    0.972\n",
      "| epoch   3 |  5500/ 1782 batches | accuracy    0.962\n",
      "| epoch   3 |  6000/ 1782 batches | accuracy    0.959\n",
      "| epoch   3 |  6500/ 1782 batches | accuracy    0.961\n",
      "| epoch   3 |  7000/ 1782 batches | accuracy    0.964\n",
      "| epoch   3 |  7500/ 1782 batches | accuracy    0.961\n",
      "| epoch   3 |  8000/ 1782 batches | accuracy    0.956\n",
      "| epoch   3 |  8500/ 1782 batches | accuracy    0.968\n",
      "| epoch   3 |  9000/ 1782 batches | accuracy    0.962\n",
      "| epoch   3 |  9500/ 1782 batches | accuracy    0.973\n",
      "| epoch   3 | 10000/ 1782 batches | accuracy    0.980\n",
      "| epoch   3 | 10500/ 1782 batches | accuracy    0.981\n",
      "| epoch   3 | 11000/ 1782 batches | accuracy    0.969\n",
      "| epoch   3 | 11500/ 1782 batches | accuracy    0.967\n",
      "| epoch   3 | 12000/ 1782 batches | accuracy    0.967\n",
      "| epoch   3 | 12500/ 1782 batches | accuracy    0.966\n",
      "| epoch   3 | 13000/ 1782 batches | accuracy    0.973\n",
      "| epoch   3 | 13500/ 1782 batches | accuracy    0.970\n",
      "| epoch   3 | 14000/ 1782 batches | accuracy    0.965\n",
      "| epoch   3 | 14500/ 1782 batches | accuracy    0.969\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  9.36s | valid accuracy    0.963\n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1782 batches | accuracy    0.956\n",
      "| epoch   4 |  1000/ 1782 batches | accuracy    0.961\n",
      "| epoch   4 |  1500/ 1782 batches | accuracy    0.960\n",
      "| epoch   4 |  2000/ 1782 batches | accuracy    0.963\n",
      "| epoch   4 |  2500/ 1782 batches | accuracy    0.963\n",
      "| epoch   4 |  3000/ 1782 batches | accuracy    0.969\n",
      "| epoch   4 |  3500/ 1782 batches | accuracy    0.968\n",
      "| epoch   4 |  4000/ 1782 batches | accuracy    0.967\n",
      "| epoch   4 |  4500/ 1782 batches | accuracy    0.968\n",
      "| epoch   4 |  5000/ 1782 batches | accuracy    0.975\n",
      "| epoch   4 |  5500/ 1782 batches | accuracy    0.965\n",
      "| epoch   4 |  6000/ 1782 batches | accuracy    0.960\n",
      "| epoch   4 |  6500/ 1782 batches | accuracy    0.964\n",
      "| epoch   4 |  7000/ 1782 batches | accuracy    0.967\n",
      "| epoch   4 |  7500/ 1782 batches | accuracy    0.961\n",
      "| epoch   4 |  8000/ 1782 batches | accuracy    0.959\n",
      "| epoch   4 |  8500/ 1782 batches | accuracy    0.970\n",
      "| epoch   4 |  9000/ 1782 batches | accuracy    0.965\n",
      "| epoch   4 |  9500/ 1782 batches | accuracy    0.974\n",
      "| epoch   4 | 10000/ 1782 batches | accuracy    0.981\n",
      "| epoch   4 | 10500/ 1782 batches | accuracy    0.982\n",
      "| epoch   4 | 11000/ 1782 batches | accuracy    0.972\n",
      "| epoch   4 | 11500/ 1782 batches | accuracy    0.968\n",
      "| epoch   4 | 12000/ 1782 batches | accuracy    0.968\n",
      "| epoch   4 | 12500/ 1782 batches | accuracy    0.968\n",
      "| epoch   4 | 13000/ 1782 batches | accuracy    0.975\n",
      "| epoch   4 | 13500/ 1782 batches | accuracy    0.971\n",
      "| epoch   4 | 14000/ 1782 batches | accuracy    0.966\n",
      "| epoch   4 | 14500/ 1782 batches | accuracy    0.970\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  9.31s | valid accuracy    0.964\n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1782 batches | accuracy    0.959\n",
      "| epoch   5 |  1000/ 1782 batches | accuracy    0.961\n",
      "| epoch   5 |  1500/ 1782 batches | accuracy    0.963\n",
      "| epoch   5 |  2000/ 1782 batches | accuracy    0.965\n",
      "| epoch   5 |  2500/ 1782 batches | accuracy    0.964\n",
      "| epoch   5 |  3000/ 1782 batches | accuracy    0.971\n",
      "| epoch   5 |  3500/ 1782 batches | accuracy    0.968\n",
      "| epoch   5 |  4000/ 1782 batches | accuracy    0.969\n",
      "| epoch   5 |  4500/ 1782 batches | accuracy    0.969\n",
      "| epoch   5 |  5000/ 1782 batches | accuracy    0.976\n",
      "| epoch   5 |  5500/ 1782 batches | accuracy    0.966\n",
      "| epoch   5 |  6000/ 1782 batches | accuracy    0.961\n",
      "| epoch   5 |  6500/ 1782 batches | accuracy    0.966\n",
      "| epoch   5 |  7000/ 1782 batches | accuracy    0.968\n",
      "| epoch   5 |  7500/ 1782 batches | accuracy    0.964\n",
      "| epoch   5 |  8000/ 1782 batches | accuracy    0.959\n",
      "| epoch   5 |  8500/ 1782 batches | accuracy    0.972\n",
      "| epoch   5 |  9000/ 1782 batches | accuracy    0.967\n",
      "| epoch   5 |  9500/ 1782 batches | accuracy    0.974\n",
      "| epoch   5 | 10000/ 1782 batches | accuracy    0.982\n",
      "| epoch   5 | 10500/ 1782 batches | accuracy    0.983\n",
      "| epoch   5 | 11000/ 1782 batches | accuracy    0.973\n",
      "| epoch   5 | 11500/ 1782 batches | accuracy    0.970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 | 12000/ 1782 batches | accuracy    0.970\n",
      "| epoch   5 | 12500/ 1782 batches | accuracy    0.969\n",
      "| epoch   5 | 13000/ 1782 batches | accuracy    0.977\n",
      "| epoch   5 | 13500/ 1782 batches | accuracy    0.972\n",
      "| epoch   5 | 14000/ 1782 batches | accuracy    0.967\n",
      "| epoch   5 | 14500/ 1782 batches | accuracy    0.971\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  9.31s | valid accuracy    0.964\n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1782 batches | accuracy    0.945\n",
      "| epoch   6 |  1000/ 1782 batches | accuracy    0.953\n",
      "| epoch   6 |  1500/ 1782 batches | accuracy    0.961\n",
      "| epoch   6 |  2000/ 1782 batches | accuracy    0.962\n",
      "| epoch   6 |  2500/ 1782 batches | accuracy    0.964\n",
      "| epoch   6 |  3000/ 1782 batches | accuracy    0.971\n",
      "| epoch   6 |  3500/ 1782 batches | accuracy    0.967\n",
      "| epoch   6 |  4000/ 1782 batches | accuracy    0.966\n",
      "| epoch   6 |  4500/ 1782 batches | accuracy    0.969\n",
      "| epoch   6 |  5000/ 1782 batches | accuracy    0.972\n",
      "| epoch   6 |  5500/ 1782 batches | accuracy    0.966\n",
      "| epoch   6 |  6000/ 1782 batches | accuracy    0.964\n",
      "| epoch   6 |  6500/ 1782 batches | accuracy    0.968\n",
      "| epoch   6 |  7000/ 1782 batches | accuracy    0.969\n",
      "| epoch   6 |  7500/ 1782 batches | accuracy    0.960\n",
      "| epoch   6 |  8000/ 1782 batches | accuracy    0.961\n",
      "| epoch   6 |  8500/ 1782 batches | accuracy    0.975\n",
      "| epoch   6 |  9000/ 1782 batches | accuracy    0.972\n",
      "| epoch   6 |  9500/ 1782 batches | accuracy    0.974\n",
      "| epoch   6 | 10000/ 1782 batches | accuracy    0.989\n",
      "| epoch   6 | 10500/ 1782 batches | accuracy    0.985\n",
      "| epoch   6 | 11000/ 1782 batches | accuracy    0.972\n",
      "| epoch   6 | 11500/ 1782 batches | accuracy    0.977\n",
      "| epoch   6 | 12000/ 1782 batches | accuracy    0.972\n",
      "| epoch   6 | 12500/ 1782 batches | accuracy    0.972\n",
      "| epoch   6 | 13000/ 1782 batches | accuracy    0.983\n",
      "| epoch   6 | 13500/ 1782 batches | accuracy    0.977\n",
      "| epoch   6 | 14000/ 1782 batches | accuracy    0.976\n",
      "| epoch   6 | 14500/ 1782 batches | accuracy    0.978\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  9.15s | valid accuracy    0.971\n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1782 batches | accuracy    0.950\n",
      "| epoch   7 |  1000/ 1782 batches | accuracy    0.963\n",
      "| epoch   7 |  1500/ 1782 batches | accuracy    0.964\n",
      "| epoch   7 |  2000/ 1782 batches | accuracy    0.967\n",
      "| epoch   7 |  2500/ 1782 batches | accuracy    0.969\n",
      "| epoch   7 |  3000/ 1782 batches | accuracy    0.978\n",
      "| epoch   7 |  3500/ 1782 batches | accuracy    0.970\n",
      "| epoch   7 |  4000/ 1782 batches | accuracy    0.970\n",
      "| epoch   7 |  4500/ 1782 batches | accuracy    0.970\n",
      "| epoch   7 |  5000/ 1782 batches | accuracy    0.976\n",
      "| epoch   7 |  5500/ 1782 batches | accuracy    0.970\n",
      "| epoch   7 |  6000/ 1782 batches | accuracy    0.969\n",
      "| epoch   7 |  6500/ 1782 batches | accuracy    0.970\n",
      "| epoch   7 |  7000/ 1782 batches | accuracy    0.971\n",
      "| epoch   7 |  7500/ 1782 batches | accuracy    0.965\n",
      "| epoch   7 |  8000/ 1782 batches | accuracy    0.965\n",
      "| epoch   7 |  8500/ 1782 batches | accuracy    0.976\n",
      "| epoch   7 |  9000/ 1782 batches | accuracy    0.972\n",
      "| epoch   7 |  9500/ 1782 batches | accuracy    0.976\n",
      "| epoch   7 | 10000/ 1782 batches | accuracy    0.989\n",
      "| epoch   7 | 10500/ 1782 batches | accuracy    0.985\n",
      "| epoch   7 | 11000/ 1782 batches | accuracy    0.974\n",
      "| epoch   7 | 11500/ 1782 batches | accuracy    0.980\n",
      "| epoch   7 | 12000/ 1782 batches | accuracy    0.973\n",
      "| epoch   7 | 12500/ 1782 batches | accuracy    0.972\n",
      "| epoch   7 | 13000/ 1782 batches | accuracy    0.983\n",
      "| epoch   7 | 13500/ 1782 batches | accuracy    0.977\n",
      "| epoch   7 | 14000/ 1782 batches | accuracy    0.976\n",
      "| epoch   7 | 14500/ 1782 batches | accuracy    0.979\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  9.25s | valid accuracy    0.973\n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1782 batches | accuracy    0.951\n",
      "| epoch   8 |  1000/ 1782 batches | accuracy    0.966\n",
      "| epoch   8 |  1500/ 1782 batches | accuracy    0.966\n",
      "| epoch   8 |  2000/ 1782 batches | accuracy    0.970\n",
      "| epoch   8 |  2500/ 1782 batches | accuracy    0.971\n",
      "| epoch   8 |  3000/ 1782 batches | accuracy    0.978\n",
      "| epoch   8 |  3500/ 1782 batches | accuracy    0.972\n",
      "| epoch   8 |  4000/ 1782 batches | accuracy    0.971\n",
      "| epoch   8 |  4500/ 1782 batches | accuracy    0.972\n",
      "| epoch   8 |  5000/ 1782 batches | accuracy    0.977\n",
      "| epoch   8 |  5500/ 1782 batches | accuracy    0.972\n",
      "| epoch   8 |  6000/ 1782 batches | accuracy    0.970\n",
      "| epoch   8 |  6500/ 1782 batches | accuracy    0.971\n",
      "| epoch   8 |  7000/ 1782 batches | accuracy    0.972\n",
      "| epoch   8 |  7500/ 1782 batches | accuracy    0.967\n",
      "| epoch   8 |  8000/ 1782 batches | accuracy    0.965\n",
      "| epoch   8 |  8500/ 1782 batches | accuracy    0.977\n",
      "| epoch   8 |  9000/ 1782 batches | accuracy    0.973\n",
      "| epoch   8 |  9500/ 1782 batches | accuracy    0.978\n",
      "| epoch   8 | 10000/ 1782 batches | accuracy    0.989\n",
      "| epoch   8 | 10500/ 1782 batches | accuracy    0.987\n",
      "| epoch   8 | 11000/ 1782 batches | accuracy    0.974\n",
      "| epoch   8 | 11500/ 1782 batches | accuracy    0.981\n",
      "| epoch   8 | 12000/ 1782 batches | accuracy    0.974\n",
      "| epoch   8 | 12500/ 1782 batches | accuracy    0.973\n",
      "| epoch   8 | 13000/ 1782 batches | accuracy    0.983\n",
      "| epoch   8 | 13500/ 1782 batches | accuracy    0.977\n",
      "| epoch   8 | 14000/ 1782 batches | accuracy    0.976\n",
      "| epoch   8 | 14500/ 1782 batches | accuracy    0.980\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  9.27s | valid accuracy    0.973\n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1782 batches | accuracy    0.952\n",
      "| epoch   9 |  1000/ 1782 batches | accuracy    0.968\n",
      "| epoch   9 |  1500/ 1782 batches | accuracy    0.967\n",
      "| epoch   9 |  2000/ 1782 batches | accuracy    0.970\n",
      "| epoch   9 |  2500/ 1782 batches | accuracy    0.972\n",
      "| epoch   9 |  3000/ 1782 batches | accuracy    0.978\n",
      "| epoch   9 |  3500/ 1782 batches | accuracy    0.973\n",
      "| epoch   9 |  4000/ 1782 batches | accuracy    0.971\n",
      "| epoch   9 |  4500/ 1782 batches | accuracy    0.973\n",
      "| epoch   9 |  5000/ 1782 batches | accuracy    0.978\n",
      "| epoch   9 |  5500/ 1782 batches | accuracy    0.972\n",
      "| epoch   9 |  6000/ 1782 batches | accuracy    0.970\n",
      "| epoch   9 |  6500/ 1782 batches | accuracy    0.971\n",
      "| epoch   9 |  7000/ 1782 batches | accuracy    0.973\n",
      "| epoch   9 |  7500/ 1782 batches | accuracy    0.968\n",
      "| epoch   9 |  8000/ 1782 batches | accuracy    0.966\n",
      "| epoch   9 |  8500/ 1782 batches | accuracy    0.977\n",
      "| epoch   9 |  9000/ 1782 batches | accuracy    0.973\n",
      "| epoch   9 |  9500/ 1782 batches | accuracy    0.979\n",
      "| epoch   9 | 10000/ 1782 batches | accuracy    0.989\n",
      "| epoch   9 | 10500/ 1782 batches | accuracy    0.987\n",
      "| epoch   9 | 11000/ 1782 batches | accuracy    0.975\n",
      "| epoch   9 | 11500/ 1782 batches | accuracy    0.981\n",
      "| epoch   9 | 12000/ 1782 batches | accuracy    0.975\n",
      "| epoch   9 | 12500/ 1782 batches | accuracy    0.974\n",
      "| epoch   9 | 13000/ 1782 batches | accuracy    0.984\n",
      "| epoch   9 | 13500/ 1782 batches | accuracy    0.977\n",
      "| epoch   9 | 14000/ 1782 batches | accuracy    0.976\n",
      "| epoch   9 | 14500/ 1782 batches | accuracy    0.979\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  9.35s | valid accuracy    0.973\n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1782 batches | accuracy    0.953\n",
      "| epoch  10 |  1000/ 1782 batches | accuracy    0.968\n",
      "| epoch  10 |  1500/ 1782 batches | accuracy    0.968\n",
      "| epoch  10 |  2000/ 1782 batches | accuracy    0.970\n",
      "| epoch  10 |  2500/ 1782 batches | accuracy    0.973\n",
      "| epoch  10 |  3000/ 1782 batches | accuracy    0.979\n",
      "| epoch  10 |  3500/ 1782 batches | accuracy    0.974\n",
      "| epoch  10 |  4000/ 1782 batches | accuracy    0.972\n",
      "| epoch  10 |  4500/ 1782 batches | accuracy    0.973\n",
      "| epoch  10 |  5000/ 1782 batches | accuracy    0.979\n",
      "| epoch  10 |  5500/ 1782 batches | accuracy    0.972\n",
      "| epoch  10 |  6000/ 1782 batches | accuracy    0.971\n",
      "| epoch  10 |  6500/ 1782 batches | accuracy    0.973\n",
      "| epoch  10 |  7000/ 1782 batches | accuracy    0.973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  10 |  7500/ 1782 batches | accuracy    0.968\n",
      "| epoch  10 |  8000/ 1782 batches | accuracy    0.966\n",
      "| epoch  10 |  8500/ 1782 batches | accuracy    0.977\n",
      "| epoch  10 |  9000/ 1782 batches | accuracy    0.973\n",
      "| epoch  10 |  9500/ 1782 batches | accuracy    0.980\n",
      "| epoch  10 | 10000/ 1782 batches | accuracy    0.990\n",
      "| epoch  10 | 10500/ 1782 batches | accuracy    0.988\n",
      "| epoch  10 | 11000/ 1782 batches | accuracy    0.976\n",
      "| epoch  10 | 11500/ 1782 batches | accuracy    0.981\n",
      "| epoch  10 | 12000/ 1782 batches | accuracy    0.975\n",
      "| epoch  10 | 12500/ 1782 batches | accuracy    0.974\n",
      "| epoch  10 | 13000/ 1782 batches | accuracy    0.984\n",
      "| epoch  10 | 13500/ 1782 batches | accuracy    0.977\n",
      "| epoch  10 | 14000/ 1782 batches | accuracy    0.976\n",
      "| epoch  10 | 14500/ 1782 batches | accuracy    0.980\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  9.25s | valid accuracy    0.973\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5 # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    #print(len(train_dataloader))\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "         'valid accuracy {:8.3f}'.format(epoch,\n",
    "                  time.time() - epoch_start_time,\n",
    "                  accu_val))\n",
    "    print('-' * 59)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58aa38d",
   "metadata": {},
   "source": [
    "## 평가 데이터로 모델 평가하기\n",
    "\n",
    "평가 데이터셋을 통한 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "826c2cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.902\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c81a4",
   "metadata": {},
   "source": [
    "## 임의의 뉴스로 평가하기\n",
    "\n",
    "현재까지 최고의 모델로 골프 뉴스를 테스트해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b27f6a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Business news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {1: \"World\",\n",
    "                2: \"Sports\",\n",
    "                3: \"Business\",\n",
    "                4: \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "# ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "# enduring the season’s worst weather conditions on Sunday at The \\\n",
    "# Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "# considering the wind and the rain was a respectable showing. \\\n",
    "# Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "# was another story. With temperatures in the mid-80s and hardly any \\\n",
    "# wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "# Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "# finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "# was even more impressive considering he’d never played the \\\n",
    "# front nine at TPC Southwind.\"\n",
    "\n",
    "ex_text_str = \"Hello, this is Test news from Dynamic Research Center. \\\n",
    "enduring the season’s economic depression, US finally decided \\\n",
    "to raise the bank rate to 4.0. This Decision is not for South Korea, \\\n",
    "so it might be a big recession next week. \\\n",
    "Stock market, especially index named KOSPI keep decreasing. \\\n",
    "Anyway, financial experts warned about this before.\"\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4db4d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
